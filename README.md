### SinLU Activation function

We propose a new non-linear trainable activation function, called Sinu-sigmoidal Linear Unit (SinLU). Here, we aim to explore the sinusoidal properties in an activation function with maintaining a ReLU-like structure. SinLU is a continuous function with a buffer zone on the negative side of the X-axis similar to GELU and SiLU. Furthermore, SinLU is trainable which means it includes some parameters that get trained during the model training which alters its shape.

![0001](https://user-images.githubusercontent.com/31564734/121135309-00ba1280-c852-11eb-819f-35bc2c2aac03.jpg =50x)
