### SinLU Activation function

We propose a new non-linear trainable activation function, called Sinu-sigmoidal Linear Unit (SinLU). Here, we aim to explore the sinusoidal properties in an activation function with maintaining a ReLU-like structure. SinLU is a continuous function with a buffer zone on the negative side of the X-axis similar to GELU and SiLU. Furthermore, SinLU is trainable which means it includes some parameters that get trained during the model training which alters its shape.
[<img align="left" width="20px" src="https://www.overleaf.com/project/607ef0edd234f977af3d3e66/file/60a3eed4a1ca9d54e89bae77"/>]