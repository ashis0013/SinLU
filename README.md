### SinLU Activation function

We propose a new non-linear trainable activation function, called Sinu-sigmoidal Linear Unit (SinLU). Here, we aim to explore the sinusoidal properties in an activation function with maintaining a ReLU-like structure. SinLU is a continuous function with a buffer zone on the negative side of the X-axis similar to GELU and SiLU. Furthermore, SinLU is trainable which means it includes some parameters that get trained during the model training which alters its shape.

[AFGraph.pdf](https://github.com/ashis0013/SinLU/files/6614004/AFGraph.pdf)
![3513670](https://user-images.githubusercontent.com/31564734/121134949-a15c0280-c851-11eb-86c7-ae796aab962f.jpg)
